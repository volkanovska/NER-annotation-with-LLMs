# Supplementary data and code 
### Workshop: NLP4Ecology
### Submission title: "Large Language Models as Annotators of Named Entities in Climate Change and Biodiversity: A Preliminary Study"

This repository contains the complete set of prompts tested in the experiments and the results obtained from each large language model (LLM).
LLMs tested:
- gpt-4o-mini (OpenAI, proprietary);
- gpt-4o-2024-05-13 (OpenAI, proprietary);
- Meta-Llama-3.1-70B-Instruct (Meta, open-source);
- Meta-Llama-3.1-405B-Instruct (Meta, open-source).
  
For prompts from the dataset **Climate-Change-NER**, the system message is: *You are a helpful climate change expert, specialized in annotating named entities in texts on climate change.*

For prompts from the dataset **BiodivNER**, the system message is: *You are a helpful biodiversity expert, specialized in annotating named entities in texts on biodiversity.*

## Saving model output

The output for each model and prompt type is stored in dedicated directories. The naming convention is: name of dataset + _ + input/output format + _ + name of model. For example, for the dataset Climate-Change-NER (ccner), token-based prompt and output format, and gpt-4o-mini model, the name of the dedicated folder is: ccner_tokens_gpt-4o-mini. 

There are *two* subdirectories within each dedicated directory, which contain the models' results in two different forms: **parsed**, where the generated text from the model has been postprocessed in a format allowing the calculation of F1 score, and **raw**, which contain the original output from each model. 

**Example structure of dedicated directory**

```bash
├── ccner_tokens_gpt-4o-mini
│   ├── parsed
│   │   ├── combination_1_4_parsed_output.json    # corresponds to NE class cluster 1
│   │   ├── combination_2_4_parsed_output.json    # corresponds to NE class cluster 2
│   │   ├── combination_3_4_parsed_output.json    # corresponds to NE class cluster 3
│   │   ├── combination_4_4_parsed_output.json    # corresponds to NE class cluster 4
│   │   ├── random_3_parsed_output.json           # a prompt with random k examples, where k == 3
│   │   ├── random_4_parsed_output.json           # a prompt with random k examples, where k == 4
│   │   ├── random_5_parsed_output.json           # a prompt with random k examples, where k == 5
│   │   ├── similarity_3_parsed_output.json       # a prompt with similar k examples, where k == 3
│   │   ├── similarity_4_parsed_output.json       # a prompt with similar k examples, where k == 4
│   │   ├── similarity_4_parsed_output.json       # a prompt with similar k examples, where k == 5
│   ├── raw
│       ├── combination_1_4_raw_output.json       # corresponds to NE class cluster 1
│       ├── combination_2_4_raw_output.json       # corresponds to NE class cluster 2
│       ├── combination_3_4_raw_output.json       # corresponds to NE class cluster 3
│       ├── combination_4_4_raw_output.json       # corresponds to NE class cluster 4
│       ├── random_3_raw_output.json              # a prompt with random k examples, where k == 3
│       ├── random_4_raw_output.json              # a prompt with random k examples, where k == 4
│       ├── random_5_raw_output.json              # a prompt with random k examples, where k == 5
│       ├── similarity_3_raw_output.json          # a prompt with similar k examples, where k == 3
│       ├── similarity_4_raw_output.json          # a prompt with similar k examples, where k == 4
│       ├── similarity_5_raw_output.json          # a prompt with similar k examples, where k == 5

```

JSON files designated as **parsed** store a list of dictionaries with identical structure, which is:

```python3
[
    {"gold_sent_str": # Gold sentence presented as a string,
     "gold_spans": [[NE instance 1, NE class, start token index, end token index], [NE instance 2, NE class, start token index, end token index] ...],
     "gold_token_labels": [[token_1, IOB-tag], [token_2, IOB-tag], [token_3, IOB-tag] ...],
     "predicted_spans": [[NE instance 1, NE class, start token index, end token index], [NE instance 2, NE class, start token index, end token index] ...],
     "predicted_token_labels": [[token_1, IOB-tag], [token_2, IOB-tag], [token_3, IOB-tag] ...],   
    },
    {
    ...
    }
]
```
JSON files designated as **raw** store a list of dictionaries with identical structure, which is:

```python3
[
    {"prompt": # Prompt sent to the LLM,
     "raw_output": # The text generated by the LLM before post-processing
    },
    {
    ...
    }
]
```

## Generating evaluation reports

Evaluation reports for each type of prompt can be recreated by running the script *generate_evaluation_report_tokens.py* for promts with token-based input, or the script *generate_evaluation_report_strings.py* for prompts with string-based input. 

For example, to obtain the results for the dataset **BiodivNER**, model **gpt-4o-mini**, and a prompt type containing **similar 5 task examples** (TEs), open a terminal and run:

```python
python3 generate_evaluation_report_tokens.py biodivner_gpt-4o-mini/parsed/similarity_5_parsed_output.json 
```

## Prompt examples

The files *Prompt_example_string_ccner_nodalida.pdf* and *Prompt_example_tokens_biodivner_nodalida.pdf* are examples of string-based and token-based prompts respectively. These files are provided for clearer overview of the prompt structure, the blueprint of which is given in Figure 1 of the paper.

# Dataset holders

**Climate-Change-NER**

Link to dataset: https://huggingface.co/datasets/ibm/Climate-Change-NER

```bibtex
@misc{bhattacharjee2024indus,
  title={INDUS: Effective and Efficient Language Models for Scientific Applications}, 
  author={Bishwaranjan Bhattacharjee and Aashka Trivedi and Masayasu Muraoka and Muthukumaran Ramasubramanian and Takuma Udagawa and Iksha Gurung and Rong Zhang and Bharath Dandala and Rahul Ramachandran and Manil Maskey and Kayleen Bugbee and Mike Little and Elizabeth Fancher and Lauren Sanders and Sylvain Costes and Sergi Blanco-Cuaresma and Kelly Lockhart and Thomas Allen and Felix Grazes and Megan Ansdel and Alberto Accomazzi and Yousef El-Kurdi and Davis Wertheimer and Birgit Pfitzmann and Cesar Berrospi Ramis and Michele Dolfi and Rafael Teixeira de Lima and Panos Vagenas and S. Karthik Mukkavilli and Peter Staar and Sanaz Vahidinia and Ryan McGranaghan and Armin Mehrabian and Tsendgar Lee},
  year={2024},
  eprint={2405.10725},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2405.10725}
}
```

**BiodivNER** 

Link to dataset: https://zenodo.org/records/6458503

```bibtex
@dataset{nora_abdelmageed_2022_6575865,
  author       = {Nora Abdelmageed and
                  Felicitas Löffler and
                  Leila Feddoul and
                  Alsayed Algergawy and
                  Sheeba Samuel and
                  Jitendra Gaikwad and
                  Anahita Kazem and
                  Birgitta König-Ries},
  title        = {{BiodivNERE: Gold Standard Corpora for Named Entity 
                   Recognition and Relation Extraction in
                   Biodiversity Domain}},
  month        = apr,
  year         = 2022,
  note         = {Added BiodivRE Multi-class corpus},
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.6575865},
  url          = {https://doi.org/10.5281/zenodo.6575865}
}
```
